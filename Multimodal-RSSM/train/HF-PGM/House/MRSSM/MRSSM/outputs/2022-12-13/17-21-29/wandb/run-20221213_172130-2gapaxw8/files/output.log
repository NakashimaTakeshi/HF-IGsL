using cuda:1
Initialize training environment and experience replay memory
load dataset from /home/docker/sharespace/Multimodal-RSSM/train/HF-PGM/House/MRSSM/MRSSM/../../../../../dataset/HF-PGM/MobileRobot_with_Image_Pose/MobileRobotImage_20221127/train
find 10 npy files!
load dataset:  50%|███████████████████████████▌                           | 5/10 [00:01<00:01,  3.40it/s]
set color augment params

load dataset: 100%|██████████████████████████████████████████████████████| 10/10 [00:03<00:00,  3.22it/s]
load dataset from /home/docker/sharespace/Multimodal-RSSM/train/HF-PGM/House/MRSSM/MRSSM/../../../../../dataset/HF-PGM/MobileRobot_with_Image_Pose/MobileRobotImage_20221127/validation
find 3 npy files!
load dataset:  33%|██████████████████▋                                     | 1/3 [00:00<00:00,  2.15it/s]
set color augment params
calc pca from torch.Size([87, 3, 258, 258]) data
Initialise model parameters randomly
load dataset: 100%|████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.59it/s]










train:   0%|                                                         | 17/8000 [00:37<4:54:12,  2.21s/it]
Error executing job with overrides: ['main.experiment_name=HF_PGM_1']
Traceback (most recent call last):
  File "main.py", line 45, in main
    run(_cfg)
  File "/home/docker/sharespace/Multimodal-RSSM/train/HF-PGM/House/MRSSM/MRSSM/../../../../../algos/MRSSM/MRSSM/train.py", line 76, in run
    train(cfg, cwd, results_dir, device)
  File "/home/docker/sharespace/Multimodal-RSSM/train/HF-PGM/House/MRSSM/MRSSM/../../../../../algos/MRSSM/MRSSM/train.py", line 61, in train
    model.optimize(D)
  File "/home/docker/sharespace/Multimodal-RSSM/train/HF-PGM/House/MRSSM/MRSSM/../../../../../algos/MRSSM/base/base.py", line 418, in optimize
    self.optimize_loss(observations_target, actions, rewards, nonterminals, states, self.itr_optim, observations_contrastive)
  File "/home/docker/sharespace/Multimodal-RSSM/train/HF-PGM/House/MRSSM/MRSSM/../../../../../algos/MRSSM/base/base.py", line 396, in optimize_loss
    self.scaler.scale(model_loss).backward()
  File "/home/docker/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/docker/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.79 GiB (GPU 1; 47.54 GiB total capacity; 13.05 GiB already allocated; 1.63 GiB free; 18.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.