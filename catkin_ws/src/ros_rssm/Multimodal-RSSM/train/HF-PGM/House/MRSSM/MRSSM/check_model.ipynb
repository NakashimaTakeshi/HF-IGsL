{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8569af-daa4-45fc-b40d-45bf2d9a37a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.join(Path().resolve(), '../../../../..'))\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ea5aec-dad2-4be5-a65e-aa5f79dca4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f79940a2f40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 勾配を計算しない\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43158ad-546c-490b-8c1a-b19172503528",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "# %matplotlib nbagg\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, ArtistAnimation\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b0b5bb-ac8e-4954-a833-941c2aa9bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"results/test2-seed_0/2022-12-07/run_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c92ba647-a962-492a-ba7c-945ddfcce2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Options\n",
      "                          main: {'experiment_name': 'test2-seed_0', 'tags': ['RSSM'], 'log_dir': '/home/docker/sharespace/Multimodal-RSSM/train/HF-PGM/House/MRSSM/MRSSM/results/test2-seed_0/2022-12-07/run_0', 'seed': 0, 'disable_cuda': False, 'device': 'cuda:0', 'wandb': True, 'git_hash': '80dccb5'}\n",
      "                          env: {'env_config': {'env_name': 'House'}, 'taskset_name': 'HF-PGM', 'observation_shapes': {'image_hsr_256': [3, 256, 256]}, 'action_name': 'Pose', 'action_size': 7, 'action_repeat': 1, 'bit_depth': 5, 'max_episode_length': None}\n",
      "                          rssm: {'observation_names_enc': ['image_hsr_256'], 'observation_names_rec': ['image_hsr_256'], 'condition_names': ['Pose'], 'predict_reward': False, 'multimodal': True, 'multimodal_params': {'fusion_timing': 'obs_emb', 'fusion_method': 'PoE', 'num_components': None}, 'MuMMI': {'use': False}, 'dreaming': {'use': False, 'cpc_overshooting_distance': 3, 'cpc_contrast': 'window', 'mycpc_temp_logits': 1}, 'coefficients': {'image_horizon': 1, 'sound': 1, 'pose_quat_norm': 1}, 'activation_function': {'cnn': 'relu', 'dense': 'elu', 'fusion': 'relu'}, 'embedding_size': {'fusion': 1024, 'image': 1024, 'sound': 256, 'other': 128}, 'hidden_size': 256, 'belief_size': 256, 'state_size': 32, 'normalization': 'BatchNorm', 'worldmodel_LogProbLoss': False, 'overshooting_distance': 0, 'overshooting_kl_beta': 0, 'overshooting_reward_scale': 0, 'global_kl_beta': 0, 'free_nats': 3, 'kl_beta': 1, 'kl_balancing_alpha': 0.5, 'obs_emb_kl_beta': 0, 'learning_rate_schedule': 0, 'adam_epsilon': 1e-07, 'grad_clip_norm': 100.0, 'model_learning_rate': 0.001}\n",
      "                          train: {'train_data_path': ['../../../../../dataset/HF-PGM/MobileRobot_with_Image_Pose/MobileRobotImage_20221127/train'], 'validation_data_path': ['../../../../../dataset/HF-PGM/MobileRobot_with_Image_Pose/MobileRobotImage_20221127/validation'], 'experience_size': 300000, 'train_iteration': 8000, 'checkpoint_interval': 1000, 'validation_interval': 10, 'test': False, 'test_interval': None, 'test_episodes': None, 'batch_size': 50, 'chunk_size': 50, 'action_noise': 0.3, 'augmentation': {'n_crop': 9, 'dh_base': 1, 'dw_base': 1, 'noise_scales': [0.0], 'pca_scales': [0.0]}, 'use_amp': True, 'model_path': None, 'pretrained_model': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134094/3059289892.py:3: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=model_folder):\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "with initialize(config_path=model_folder):\n",
    "    cfg = compose(config_name=\"hydra_config\")\n",
    "\n",
    "cfg_device = \"cuda:0\"\n",
    "# cfg_device = \"cpu\"\n",
    "cfg.main.device = cfg_device\n",
    "# cfg.train.n_episode = 100\n",
    "print(' ' * 26 + 'Options')\n",
    "for k, v in cfg.items():\n",
    "    print(' ' * 26 + k + ': ' + str(v))\n",
    "\n",
    "device = torch.device(cfg.main.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdddd872-9c9a-403a-bab5-e78c539caf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.emv.action_name = \"action\"\n",
    "# cfg.rssm.condition_names = [\"action\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fa3016-6a17-45b1-9ead-7a38741da969",
   "metadata": {},
   "source": [
    "# Load Model, Data and States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b49214c-97c7-48a8-a2df-11c894461ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_pathes: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['results/test2-seed_0/2022-12-07/run_0/models_1000.pth',\n",
       " 'results/test2-seed_0/2022-12-07/run_0/models_2000.pth',\n",
       " 'results/test2-seed_0/2022-12-07/run_0/models_3000.pth',\n",
       " 'results/test2-seed_0/2022-12-07/run_0/models_4000.pth',\n",
       " 'results/test2-seed_0/2022-12-07/run_0/models_5000.pth',\n",
       " 'results/test2-seed_0/2022-12-07/run_0/models_6000.pth',\n",
       " 'results/test2-seed_0/2022-12-07/run_0/models_7000.pth',\n",
       " 'results/test2-seed_0/2022-12-07/run_0/models_8000.pth']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "model_paths = glob.glob(os.path.join(model_folder, '*.pth'))\n",
    "print(\"model_pathes: \")\n",
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff0ea345-c6b6-4204-a956-f0f32df00b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal RSSM (fusion obs_emb by PoE)\n",
      "load model_dicts from results/test2-seed_0/2022-12-07/run_0/models_4000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/optim/adam.py:90: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(Adam, self).__init__(params, defaults)\n"
     ]
    }
   ],
   "source": [
    "model_idx = 3\n",
    "\n",
    "from algos.MRSSM.MRSSM.algo import build_RSSM\n",
    "\n",
    "model = build_RSSM(cfg, device)\n",
    "model_path = model_paths[model_idx]\n",
    "model.load_model(model_path)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028f632e-62fc-4c27-a3b1-6010dbaba3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_validation_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "283d2cab-84ff-4d6b-9f93-fd7e5c24e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset from ./../../../../../dataset/HF-PGM/MobileRobot_with_Image_Pose/MobileRobotImage_20221127/validation\n",
      "find 3 npy files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set color augment params\n",
      "calc pca from torch.Size([87, 3, 258, 258]) data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from algos.MRSSM.MRSSM.train import get_dataset_loader\n",
    "\n",
    "cwd = \".\"\n",
    "if use_validation_data:\n",
    "    D = get_dataset_loader(cfg, cwd, device, cfg.train.validation_data_path)\n",
    "else:\n",
    "    D = get_dataset_loader(cfg, cwd, device, cfg.train.train_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2650c5-bc2c-471e-8e70-a9c1cfee6668",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f0ed565-acd6-4ef1-a52a-74fdf6d67a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation.estimate_states import get_episode_data\n",
    "\n",
    "epi_idx = 0\n",
    "crop_idx = 0\n",
    "observations, actions, rewards, nonterminals = get_episode_data(D, epi_idx=epi_idx, crop_idx=crop_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08603f33-ea2b-4ec0-ba7a-e5e6d24f34e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7829c175-9e5f-4f0d-9eca-6fd95cb72255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image_hsr_256'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdaf6382-aa8e-4e3c-bd54-4338755b27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256,256)\n",
    "size = (64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89a3d8d1-8ecf-411e-b5fd-afae347e2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/docker/.pyenv/versions/3.8.0/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/docker/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from scikit-learn) (1.9.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/docker/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/docker/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from scikit-learn) (1.23.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/docker/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada02a1c-52dc-4ab1-a640-bf9ab5b10cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20678a6a-e88c-4f64-b752-4c46ad454e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c83e4-2db5-494f-a82c-52ad3ad1cb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bce3238-fa6e-4c02-9dfa-70065591650f",
   "metadata": {},
   "source": [
    "### get PCA result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d69fe846-0099-4be7-b89b-4b19e122f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation.visualize_utils import get_pca_model, tensor2np, get_xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d361f1-f295-477d-9edd-de6cf18b7ac9",
   "metadata": {},
   "source": [
    "#### <span style=\"color: red; \">please run estimate_state.py for estimate states of train dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b16aaa18-cef2-4317-a40c-f9af417ff421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load states\n",
    "# state_path = model_path.replace(\"models\", \"states_models\").replace(\".pth\", \".npy\")\n",
    "# print(\"state_path:\", state_path)\n",
    "\n",
    "# states_np = np.load(state_path, allow_pickle=True).item()\n",
    "# # print(\"-- dataset --\")\n",
    "# # for key in states_np.keys():\n",
    "# #     print(key)\n",
    "\n",
    "# # print(\"-- key of states --\")\n",
    "# # print(states_np[key].keys())\n",
    "\n",
    "# ht = [states_np[key][\"beliefs\"] for key in states_np.keys()]\n",
    "# ht = np.vstack(ht)\n",
    "# pca_ht = get_pca_model(ht)\n",
    "\n",
    "# st_q = [states_np[key][\"posterior_means\"] for key in states_np.keys()]\n",
    "# st_q = np.vstack(st_q)\n",
    "# pca_st_q = get_pca_model(st_q)\n",
    "\n",
    "# colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \"tab:brown\", \"tab:pink\", \"tab:gray\", \"tab:olive\", \"tab:cyan\"]\n",
    "\n",
    "# fig = plt.figure(figsize=(6, 6))\n",
    "# # ax = fig.add_subplot(111, projection=\"3d\")\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# for key in states_np.keys():\n",
    "#     num = int(key.split(\"/\")[-2])\n",
    "#     color = colors[num]\n",
    "    \n",
    "#     ht = states_np[key][\"beliefs\"]\n",
    "#     shape = ht.shape[-1]\n",
    "#     feat = tensor2np(ht).reshape(-1, shape)\n",
    "#     feat_pca = pca_ht.transform(feat)\n",
    "#     x, y, z = get_xyz(feat_pca)\n",
    "#     ax.plot(x,y,z, alpha=0.2, color=color)\n",
    "#     # ax.scatter(x, y, z, alpha=0.2)\n",
    "# plt.show()\n",
    "\n",
    "# fig = plt.figure(figsize=(6, 6))\n",
    "# # ax = fig.add_subplot(111, projection=\"3d\")\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# for key in states_np.keys():\n",
    "#     num = int(key.split(\"/\")[-2])\n",
    "#     color = colors[num]\n",
    "    \n",
    "#     st_q = states_np[key][\"posterior_means\"]\n",
    "#     shape = st_q.shape[-1]\n",
    "#     feat = tensor2np(st_q).reshape(-1, shape)\n",
    "#     feat_pca = pca_st_q.transform(feat)\n",
    "#     x, y, z = get_xyz(feat_pca)\n",
    "#     ax.plot(x, y, alpha=0.2, color=color)\n",
    "#     # ax.scatter(x, y, z, alpha=0.2)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf0519-518f-44ee-a64d-348b3f38a296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ca104-6593-46cb-9e37-265606f7fbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6361f860-72f8-4eff-a625-f80ceb6afaf8",
   "metadata": {},
   "source": [
    "### reconstruction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "583dc2ed-d6aa-4838-9feb-47fc9c91dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_target = model._clip_obs(observations, idx_start=1)\n",
    "state = model.estimate_state(observations_target, actions[:-1], rewards, nonterminals[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa1d8d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0451e+01, 1.0000e-01, 3.6013e+00, 9.2809e+01, 2.2434e+02,\n",
      "          1.8348e+02, 1.0516e+02, 1.0000e-01, 1.7196e+02, 1.4635e+02,\n",
      "          1.0000e-01, 1.0000e-01, 1.8327e+01, 1.6882e+02, 1.0000e-01,\n",
      "          1.0000e-01, 1.3697e+02, 1.4408e+02, 1.5743e+02, 1.0000e-01,\n",
      "          1.8145e+02, 1.0001e-01, 4.3007e+01, 2.1280e+02, 1.0001e-01,\n",
      "          1.0000e-01, 1.2953e+02, 1.6238e+02, 1.9647e+02, 8.6683e+01,\n",
      "          1.8160e+02, 1.0000e-01]],\n",
      "\n",
      "        [[1.0941e+01, 1.0000e-01, 3.4711e+00, 9.4146e+01, 2.2500e+02,\n",
      "          1.8422e+02, 1.0552e+02, 1.0000e-01, 1.7242e+02, 1.4732e+02,\n",
      "          1.0000e-01, 1.0000e-01, 1.9881e+01, 1.6837e+02, 1.0000e-01,\n",
      "          1.0000e-01, 1.3728e+02, 1.4525e+02, 1.5773e+02, 1.0000e-01,\n",
      "          1.8137e+02, 1.0001e-01, 4.3098e+01, 2.1270e+02, 1.0002e-01,\n",
      "          1.0000e-01, 1.3041e+02, 1.6274e+02, 1.9653e+02, 8.6102e+01,\n",
      "          1.8194e+02, 1.0000e-01]],\n",
      "\n",
      "        [[1.1066e+01, 1.0000e-01, 3.5666e+00, 9.4783e+01, 2.2544e+02,\n",
      "          1.8397e+02, 1.0575e+02, 1.0000e-01, 1.7223e+02, 1.4792e+02,\n",
      "          1.0000e-01, 1.0000e-01, 2.0323e+01, 1.6804e+02, 1.0000e-01,\n",
      "          1.0000e-01, 1.3717e+02, 1.4558e+02, 1.5777e+02, 1.0000e-01,\n",
      "          1.8172e+02, 1.0000e-01, 4.2709e+01, 2.1328e+02, 1.0001e-01,\n",
      "          1.0000e-01, 1.3085e+02, 1.6330e+02, 1.9708e+02, 8.5637e+01,\n",
      "          1.8228e+02, 1.0000e-01]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(state[\"posterior_std_devs\"][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f93f04b9-1423-46d6-afb5-00df2f865882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['beliefs', 'prior_states', 'prior_means', 'prior_std_devs', 'posterior_states', 'posterior_means', 'posterior_std_devs'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70cef1dd-b833-4524-b270-fc53e77fb725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image_hsr_256'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recon = model.rssm.observation_model(h_t=state[\"beliefs\"], s_t=state[\"posterior_states\"])\n",
    "recon = model.observation_model(h_t=state[\"beliefs\"][:1], s_t=state[\"posterior_means\"][:1])\n",
    "recon.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "499740db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           ...,\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan],\n",
      "           [nan, nan, nan,  ..., nan, nan, nan]]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(recon['image_hsr_256']['loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b49f86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.isnan(state[\"posterior_means\"][:1]).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d9e23f2-2dc8-4110-a259-dab01abaee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# from utils.evaluation.visualize_utils import reverse_image_observation\n",
    "# from IPython import display\n",
    "# import mpl_toolkits\n",
    "\n",
    "# h_graph = 1\n",
    "# w_graph = 2\n",
    "# fig = plt.figure(figsize=(w_graph*5,h_graph*5))\n",
    "# ax1 = fig.add_subplot(h_graph, w_graph, 1)\n",
    "# ax2 = fig.add_subplot(h_graph, w_graph, 2)\n",
    "\n",
    "\n",
    "# obs_name1 = \"image_bin\"\n",
    "\n",
    "# n_frame = len(recon[obs_name1][\"loc\"])\n",
    "\n",
    "# ax1.axis(\"off\")\n",
    "# ax2.axis(\"off\")\n",
    "\n",
    "# colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \"tab:brown\", \"tab:pink\", \"tab:gray\", \"tab:olive\", \"tab:cyan\"]\n",
    "\n",
    "\n",
    "# dt = 1\n",
    "# size = (64,64)\n",
    "# artists = []\n",
    "# for t in range(0, n_frame, dt):\n",
    "#     im = observations_target[obs_name1][t, 0].detach().cpu().numpy()\n",
    "#     im = reverse_image_observation(im)\n",
    "#     im = cv2.resize(im, size, interpolation=cv2.INTER_LINEAR)\n",
    "#     im1 = ax1.imshow(im, cmap=\"gray\")\n",
    "#     ax1.set_title(\"Observation\")\n",
    "    \n",
    "#     im = recon[obs_name1][\"loc\"][t, 0].detach().cpu().numpy()\n",
    "#     im = reverse_image_observation(im)\n",
    "#     im = cv2.resize(im, size, interpolation=cv2.INTER_LINEAR)\n",
    "#     im2 = ax2.imshow(im, cmap=\"gray\")\n",
    "#     ax2.set_title(\"Reconstruction\")\n",
    "    \n",
    "#     artists.append([im1,im2])\n",
    "\n",
    "# # 4. アニメーション化\n",
    "# anim = ArtistAnimation(fig, tqdm(artists), interval=100*dt)\n",
    "\n",
    "# if use_validation_data:\n",
    "#     folder_name = \"figs/validation\"\n",
    "# else:\n",
    "#     folder_name = \"figs/train\"\n",
    "\n",
    "# os.makedirs(folder_name, exist_ok=True)\n",
    "# save_file_name = \"{}/reconstruction_ep{}.mp4\".format(folder_name, epi_idx)\n",
    "# anim.save(save_file_name, writer='ffmpeg')\n",
    "# plt.show()\n",
    "\n",
    "# print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a28a6-7579-4234-ba7a-5253e6645986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91ec7dfb-1773-4128-a6b8-58012ea07dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "import cv2\n",
    "from torch import zeros\n",
    "from utils.evaluation.visualize_utils import reverse_image_observation\n",
    "from IPython import display\n",
    "import mpl_toolkits\n",
    "\n",
    "\n",
    "obs_name1 = \"image_hsr_256\"\n",
    "\n",
    "n_frame = len(recon[obs_name1][\"loc\"])\n",
    "\n",
    "colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \"tab:brown\", \"tab:pink\", \"tab:gray\", \"tab:olive\", \"tab:cyan\"]\n",
    "\n",
    "h_graph = 1\n",
    "w_graph = 2\n",
    "\n",
    "dt = 1\n",
    "size = (64,64)\n",
    "\n",
    "\n",
    "def plot_1frame(t=0):\n",
    "    fig = plt.figure(figsize=(w_graph*5,h_graph*5))\n",
    "    ax1 = fig.add_subplot(h_graph, w_graph, 1)\n",
    "    ax2 = fig.add_subplot(h_graph, w_graph, 2)\n",
    "    \n",
    "    ax1.axis(\"off\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "        \n",
    "    im = observations_target[obs_name1][t, 0][[2,1,0]].detach().cpu().numpy()\n",
    "    im = reverse_image_observation(im)\n",
    "    im = cv2.resize(im, size, interpolation=cv2.INTER_LINEAR)\n",
    "    im1 = ax1.imshow(im)\n",
    "    ax1.set_title(\"Observation\")\n",
    "    \n",
    "    im = recon[obs_name1][\"loc\"][t, 0][[2,1,0]].detach().cpu().numpy()\n",
    "    im = reverse_image_observation(im)\n",
    "    im = cv2.resize(im, size, interpolation=cv2.INTER_LINEAR)\n",
    "    im2 = ax2.imshow(im)\n",
    "    ax2.set_title(\"Reconstruction\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69545588-b3fe-4be5-be56-ca5058cdf829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec8e4ae597b4222b4f5d7035e8f455f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='t', max=2966), Output()), _dom_classes=('widget-interact…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "interact(plot_1frame, t=(0,n_frame-1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85082636-34ed-48a8-a5ef-36d4cb86fe9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3686ca5-4bd2-4862-ba5d-229c324d52e3",
   "metadata": {},
   "source": [
    "# Imagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29325e6c-413f-4c18-9ec2-9a006035938a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65300a12-e20b-4dc8-9905-69104ca3ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_imag_start = 50\n",
    "\n",
    "h_t = [state[\"beliefs\"][t_imag_start]]\n",
    "# s_t = [state[\"posterior_states\"][t_imag_start]]\n",
    "s_t = [state[\"posterior_means\"][t_imag_start]]\n",
    "\n",
    "t_max = len(actions)\n",
    "horizon_imagination = t_max-t_imag_start\n",
    "for t in range(horizon_imagination):\n",
    "    belief, _, prior_mean, _ = model.transition_model(s_t[t], actions[t_imag_start+t].unsqueeze(dim=0), h_t[t], det=True)\n",
    "    h_t.append(belief.squeeze(dim=0))\n",
    "    # st.append(prior_state.squeeze(dim=0))\n",
    "    s_t.append(prior_mean.squeeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9fccd71-e9ec-4851-8465-4c8c5e92370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t = torch.stack(h_t)\n",
    "s_t = torch.stack(s_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12bf2111-7fe3-4240-87cb-dd3c733b8684",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_imag = model.observation_model(h_t=h_t, s_t=s_t)\n",
    "# recon_imag[\"image_vertical_high_resolution\"][\"loc\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "939d1494-a691-4430-86e9-77ac2f227013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# from utils.evaluation.visualize_utils import reverse_image_observation\n",
    "# from IPython import display\n",
    "# import mpl_toolkits\n",
    "\n",
    "# h_graph = 1\n",
    "# w_graph = 2\n",
    "# fig = plt.figure(figsize=(w_graph*5,h_graph*5))\n",
    "# ax1 = fig.add_subplot(h_graph, w_graph, 1)\n",
    "# ax2 = fig.add_subplot(h_graph, w_graph, 2)\n",
    "\n",
    "\n",
    "# obs_name1 = \"image_hsr_256\"\n",
    "\n",
    "# n_frame = horizon_imagination\n",
    "\n",
    "# ax1.axis(\"off\")\n",
    "# ax2.axis(\"off\")\n",
    "\n",
    "# colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \"tab:brown\", \"tab:pink\", \"tab:gray\", \"tab:olive\", \"tab:cyan\"]\n",
    "\n",
    "\n",
    "# dt = 1\n",
    "# size = (64,64)\n",
    "# artists = []\n",
    "# for t in range(0, n_frame, dt):\n",
    "#     t_obs = t_imag_start+t\n",
    "    \n",
    "#     if t_obs >= len(observations[obs_name1]):\n",
    "#         t_obs = len(observations[obs_name1])-1\n",
    "    \n",
    "#     im = observations[obs_name1][t_obs, 0].detach().cpu().numpy()\n",
    "#     im = reverse_image_observation(im)\n",
    "#     im = cv2.resize(im, size, interpolation=cv2.INTER_LINEAR)\n",
    "#     im1 = ax1.imshow(im, cmap=\"gray\")\n",
    "#     ax1.set_title(\"Observation\")\n",
    "    \n",
    "#     im = recon_imag[obs_name1][\"loc\"][t, 0].detach().cpu().numpy()\n",
    "#     im = reverse_image_observation(im)\n",
    "#     im = cv2.resize(im, size, interpolation=cv2.INTER_LINEAR)\n",
    "#     im2 = ax2.imshow(im, cmap=\"gray\")\n",
    "#     ax2.set_title(\"Imagination\")\n",
    "    \n",
    "#     artists.append([im1,im2])\n",
    "\n",
    "\n",
    "# # 4. アニメーション化\n",
    "# anim = ArtistAnimation(fig, tqdm(artists), interval=100*dt)\n",
    "\n",
    "# if use_validation_data:\n",
    "#     folder_name = \"figs/validation\"\n",
    "# else:\n",
    "#     folder_name = \"figs/train\"\n",
    "\n",
    "# os.makedirs(folder_name, exist_ok=True)\n",
    "# save_file_name = \"{}/imagination_ep{}.mp4\".format(folder_name, epi_idx)\n",
    "# anim.save(save_file_name, writer='ffmpeg')\n",
    "# plt.show()\n",
    "\n",
    "# print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aca591a4-a434-4a1b-a4a0-ee781b123912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "import cv2\n",
    "from torch import zeros\n",
    "from utils.evaluation.visualize_utils import reverse_image_observation\n",
    "from IPython import display\n",
    "import mpl_toolkits\n",
    "\n",
    "\n",
    "obs_name1 = \"image_hsr_256\"\n",
    "\n",
    "colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \"tab:brown\", \"tab:pink\", \"tab:gray\", \"tab:olive\", \"tab:cyan\"]\n",
    "\n",
    "h_graph = 1\n",
    "w_graph = 2\n",
    "\n",
    "dt = 1\n",
    "size = (64,64)\n",
    "\n",
    "\n",
    "def plot_1frame(t=0):\n",
    "    fig = plt.figure(figsize=(w_graph*5,h_graph*5))\n",
    "    ax1 = fig.add_subplot(h_graph, w_graph, 1)\n",
    "    ax2 = fig.add_subplot(h_graph, w_graph, 2)\n",
    "    \n",
    "    ax1.axis(\"off\")\n",
    "    ax2.axis(\"off\")\n",
    "    t_obs = t_imag_start+t\n",
    "    \n",
    "    if t_obs >= len(observations[obs_name1]):\n",
    "        t_obs = len(observations[obs_name1])-1\n",
    "    \n",
    "    im = observations[obs_name1][t_obs, 0][[2,1,0]].detach().cpu().numpy()\n",
    "    im = reverse_image_observation(im)\n",
    "    im = cv2.resize(im, size, interpolation=cv2.INTER_LINEAR)\n",
    "    im1 = ax1.imshow(im)\n",
    "    ax1.set_title(\"Observation\")\n",
    "    \n",
    "    im = recon_imag[obs_name1][\"loc\"][t, 0][[2,1,0]].detach().cpu().numpy()\n",
    "    im = reverse_image_observation(im)\n",
    "    im = cv2.resize(im, size, interpolation=cv2.INTER_LINEAR)\n",
    "    im2 = ax2.imshow(im)\n",
    "    ax2.set_title(\"Imagination\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef2b901c-5e48-4ed1-891d-e1ea79f9fe2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10dcbc50b0f4b6f85f0e5a250324b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='t', max=2917), Output()), _dom_classes=('widget-interact…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "n_frame = horizon_imagination\n",
    "interact(plot_1frame, t=(0,n_frame-1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300789d1-1930-4589-9587-e77fcd0f436b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
